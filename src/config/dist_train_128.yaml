# split train.csv
k_folds: 5

backbone: tf_efficientnet_b3
optimizer: Ranger
momentum: 0.9
batch_size: 128
accumulated_gradient: 2
epochs: 20
weight_decay: 2e-4
max_grad_norm: 10
print_freq: 50
model_suffix: '_128'

transform: "strong_fix2"
p: 0.9 # for transform
# criterion: BiTemperedLoss
# t1: 0.8
# t2: 1.5
criterion: TaylorCrossEntropyLoss
smoothing: 0.1

# scheduler: GradualWarmupScheduler
# total_epoch: 10
# after_scheduler: CosineAnnealingLR
scheduler: CosineAnnealingLR
lr: 1e-3
min_lr: 1e-6

amp: False
apex: False
DDP: True
norm_confusion_matrix: True
num_workers: 6